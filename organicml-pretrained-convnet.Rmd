---
title: "Organic ML Pretrained Convnet"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load the libraries

```{r}
library(keras)
```

## Define where the image datasets are

There is a small dataset of 410 images. They are split like this:

|dataset|benzene ring images|non benzene ring images
|---|---|---|
|train|125|125|
|validation|25|25|
|test|55|55|

```{r}
train_dir <- "data/train"
validation_dir <- "data/validation"
test_dir <- "data/test"
```

## Create the image generators

Use data augmentation on the training set, but not the validation set.

Create a function to create the generators. This function will be called twice. The first time will be during training of the first network. The second time will be training the second network.

```{r warning=FALSE}
create_generators <- function(test_validation_batch_size) {
  validation_datagen <- image_data_generator(rescale = 1/255)

  # train_datagen <- image_data_generator(
  #   rescale = 1/255,
  #   rotation_range = 90,
  #   width_shift_range = 0.01,
  #   height_shift_range = 0.01,
  #   shear_range = 0.2,
  #   zoom_range = 0.01,
  #   horizontal_flip = TRUE
  # )
  
  train_datagen <- image_data_generator(rescale = 1/255)
  
  train_generator <- flow_images_from_directory(
    train_dir,
    train_datagen,
    target_size = c(256, 256),
    batch_size = test_validation_batch_size,
    class_mode = "binary"
  )
  
  validation_generator <- flow_images_from_directory(
    validation_dir,
    validation_datagen,
    target_size = c(256, 256),
    batch_size = test_validation_batch_size,
    class_mode = "binary"
  )
  
  list(train_generator = train_generator, validation_generator = validation_generator)
}
```

## Define the network

Use VGG16 trained on imagenet for the convolutional layers. Freeze the convolutional layers so that their weights are not adjusted during training. The final layers will be a simple binary classifier made with dense layers.

First, make a function that can return the same network set up multiple times. It will be called twice: during the initial training and after the optimal number of epochs is reached.

```{r}
create_network <- function() {
  conv_base <- application_vgg16(
    weights = "imagenet",
    include_top = FALSE,
    input_shape = c(256, 256, 3)
  )
  
  conv_base %>% freeze_weights()
  
  network <- keras_model_sequential() %>%
    conv_base %>%
    layer_flatten() %>%
    layer_dense(units = 256, activation = "relu") %>%
    layer_dense(units = 128, activation = "relu") %>%
    layer_dense(units = 64, activation = "relu") %>%
   layer_dense(units = 1, activation = "sigmoid")
  
  network %>% compile(
    loss = "binary_crossentropy",
    optimizer = optimizer_rmsprop(learning_rate = 2e-5),
    metrics = c("accuracy")
  )
  
  network
}
```

Now use the function to create the network

```{r error=FALSE, warning=FALSE}
convnet_1 <- create_network()
summary(convnet_1)
```

## Fit the network using the training and validation data

The `steps_per_epoch` are calculated from the image generator batch size, according to the following StackOverflow post:

[https://stackoverflow.com/questions/60509425/how-to-use-repeat-function-when-building-data-in-keras](https://stackoverflow.com/questions/60509425/how-to-use-repeat-function-when-building-data-in-keras)


```{r error=FALSE, results='hide'}
test_validation_batch_size <- 25
generators_1 <- create_generators(test_validation_batch_size = test_validation_batch_size)
fit_history_1 <- convnet_1 %>%
  fit(
    generators_1$train_generator,
    steps_per_epoch = 250/test_validation_batch_size,
    epochs = 25,
    validation_data = generators_1$validation_generator,
    validation_steps = 50/test_validation_batch_size
  )
```

## Analyze training hisotry

```{r}
plot(fit_history_1)
```

What epoch had the maximum validation accuracy?

```{r}
max_val_accuracy_1 <- max(fit_history_1$metrics$val_accuracy)
argmax_val_accuracy_1 <- which.max(fit_history_1$metrics$val_accuracy)
cat("Maximum val_accuracy: ", max_val_accuracy_1, "\n")
cat("Epoch of maximum validation accuracy: ", argmax_val_accuracy_1, "\n")
```

## Retrain for the optimum number of epochs

Start everything over with a new network and new generators.

```{r error=FALSE, results='hide'}
convnet_2 <- create_network()
test_validation_batch_size <- 25
generators_2 <- create_generators(test_validation_batch_size = test_validation_batch_size)
fit_history_2 <- convnet_2 %>%
  fit(
    generators_2$train_generator,
    steps_per_epoch = 250/test_validation_batch_size,
    epochs = argmax_val_accuracy_1,
    validation_data = generators_2$validation_generator,
    validation_steps = 50/test_validation_batch_size
  )
```

Plot the results with the new network.

```{r}
plot(fit_history_2)
```

Analyze the results of the new network

```{r}
max_val_accuracy_2 <- max(fit_history_2$metrics$val_accuracy)
argmax_val_accuracy_2 <- which.max(fit_history_2$metrics$val_accuracy)
cat("Maximum val_accuracy: ", max_val_accuracy_2, "\n")
cat("Epoch of maximum validation accuracy: ", argmax_val_accuracy_2, "\n")
```

```{r echo=FALSE}
convnet_2 %>% save_model_hdf5("convnet_and_classifier.h5")
```
